{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9027902790279028\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import  jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "from random import choices\n",
    "from numpy import genfromtxt\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "a=0.001\n",
    "b_1=0.9\n",
    "b_2=0.999\n",
    "e=10**-7\n",
    "Nt=500                    # number of iterations\n",
    "batch=32                      # batch size\n",
    "los_1=[]\n",
    "N=500;\n",
    "n1=50;\n",
    "#n2=50;\n",
    "#n3=100;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_size():\n",
    "    indices=choices(range(0,training.shape[0]),k=batch)\n",
    "    x_1=x_m[indices]\n",
    "    y_1=y_m[indices]\n",
    "    return x_1.T, y_1\n",
    "#geenrate initial random weights\n",
    "    \n",
    "#normalise the training data xs\n",
    "def norm_weights(z):\n",
    "    mean = np.mean(z[:,:-1],axis=0)[None,:];\n",
    "    variance = np.var(z[:,:-1],axis=0)[None,:];\n",
    "    x1=np.divide((z[:,:-1]-mean),np.sqrt(variance+e));\n",
    "    qq=np.hstack((x1,z[:,-1][:,None]))\n",
    "    return qq\n",
    "\n",
    "def l2_norm(x,weight1,weight2,bias1):\n",
    "    a1=np.matmul(weight1,x)+bias1;\n",
    "    y1=np.maximum(a1,0);\n",
    "    a2=np.matmul(weight2,y1);\n",
    "    y4=1/(1+np.exp(-a2));\n",
    "    cost = -(1.0/32) * np.sum(y.T*np.log(y4) + (1-y.T)*np.log(1-y4))\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "  return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "def generate_rand_w():\n",
    "    weight1 = np.random.normal(0,2/(12+n1),(n1,12));\n",
    "    weight2 = np.random.normal(0,2/(n1+1),(1,n1));\n",
    "\n",
    "    bias1 = np.random.random_sample([n1,1]);\n",
    "\n",
    "    return weight1, weight2, bias1\n",
    "\n",
    "\n",
    "my_data = genfromtxt('data.csv', delimiter=',')[1:,0:-1]        #extract all elements except the first row and the last column\n",
    "training = my_data[0:6666,:]                                    # using the first 2/3 of the data for training\n",
    "testing = my_data[6667:,:]\n",
    "training = norm_weights(training);\n",
    "x_m=training[:,:-1];\n",
    "y_m=(np.sign(training[:,-1][:,None])*2+2)/4\n",
    "#x_1=my_data[0:6666,:-1]\n",
    "#y=my_data[0:6666,-1]\n",
    "\n",
    "\n",
    "\n",
    "weight1, weight2, bias1 =generate_rand_w();\n",
    "\n",
    "vw1=0;\n",
    "\n",
    "mw2=0;\n",
    "vw2=0;\n",
    "mw1=0;\n",
    "mb1=0;\n",
    "vb1=0;\n",
    "\n",
    "qy=[];\n",
    "\n",
    "\n",
    "jacobian_w2=jacobian(l2_norm,2);\n",
    "\n",
    "jacobian_w1=jacobian(l2_norm,1);\n",
    "jacobian_b1=jacobian(l2_norm,3);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for t in range(1,40):\n",
    "    for q in range(1,500):\n",
    "        x,y=batch_size();\n",
    "        \n",
    "        \n",
    "\n",
    "        wu2=np.reshape(jacobian_w2(x,weight1,weight2,bias1),(1,n1));\n",
    "        wu1=np.reshape(jacobian_w1(x,weight1,weight2,bias1),(n1,12));\n",
    "        b1 =np.reshape(jacobian_b1(x,weight1,weight2,bias1),(n1,1));\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        mw2 = b_1*mw2 + (1-b_1)*wu2;\n",
    "        vw2 = b_2*vw2 + (1-b_2)*np.power(wu2,2);\n",
    "          \n",
    "        mw2t=mw2/(1-np.power(b_1,t));\n",
    "        vw2t=vw2/(1-np.power(b_2,t));\n",
    "        \n",
    "        mw1 = b_1*mw1 + (1-b_1)*wu1;\n",
    "        vw1 = b_2*vw1 + (1-b_2)*np.power(wu1,2);\n",
    "          \n",
    "        mw1t=mw1/(1-np.power(b_1,t));\n",
    "        vw1t=vw1/(1-np.power(b_2,t));\n",
    "        \n",
    "        mb1 = b_1*mb1 + (1-b_1)*b1;\n",
    "        vb1 = b_2*vb1 + (1-b_2)*np.power(b1,2);\n",
    "          \n",
    "        mb1t=mb1/(1-np.power(b_1,t));\n",
    "        vb1t=vb1/(1-np.power(b_2,t));\n",
    "        \n",
    "\n",
    "\n",
    "        weight2 = weight2 - a*np.divide(mw2t,np.sqrt(vw2t)+e);\n",
    "        weight1 = weight1 - a*np.divide(mw1t,np.sqrt(vw1t)+e);\n",
    "        bias1=bias1 - a*np.divide(mb1t,np.sqrt(vb1t)+e);\n",
    "\n",
    "        qy=np.append(qy,l2_norm(x,weight1,weight2,bias1));\n",
    "    #print(l2_norm(x,weight1,weight2,bias1),'.')        \n",
    "#plt.plot(qy)\n",
    "\n",
    "\n",
    "testing=norm_weights(testing);\n",
    "x_t=testing[:,:-1];\n",
    "y_t=((np.sign(testing[:,-1])+1)/2)[:,None]\n",
    "a1=np.matmul(weight1,x_t.T)+bias1;\n",
    "y1=np.maximum(a1,0);\n",
    "a2=np.matmul(weight2,y1);\n",
    "\n",
    "\n",
    "y4=1/(1+np.exp(-a2));\n",
    "y_test_p = (np.sign(y4-0.5)+1)/2\n",
    "z = sk.confusion_matrix(y_t,y_test_p.T)\n",
    "den=np.sum(z)\n",
    "num=np.trace(z)\n",
    "accuracy=num/den\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
